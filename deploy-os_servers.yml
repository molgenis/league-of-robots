#
#   1.   $> cd git/league-of-robots
# Create Python virtual environment (once)
#   2.   $> python3 -m venv openstacksdk.venv
# Activate virtual environment.
#   3.   $> source openstacksdk.venv/bin/activate
# Install OpenStack SDK (once).
#   4.   $> pip3 install openstacksdk
#        $> pip3 install ruamel.yaml
# NOTE: Openstack RC file must be sourced first to be able to use Openstack API from SDK:
#   5. Login to OpenStack web interface -> "Identity" -> "Application Credentials" -> click the "Create Application Credential" button.
#      This will result in a popup window: specify "Name", "Expiration Date", "Expiration Time", leave the rest empty / use defaults and
#      click the "Create Application Credential" button.
#      In the new popup window click the "Download openrc file" button and save the generated *-openrc.sh file in the root of the repo.
#   6. Source the downloaded file. E.g.:
#        $> source ./[Application_Credential_Name]-openrc.sh
#   7. Fetch Ansible dependencies
#        $> ansible-galaxy install -r galaxy-requirements.yml
# Configure this repo for deployment of a specifc HPC cluster.
#   8. Source lor-init from this repo. E.g.:
#        $> source ./lor-init
#   9. Configure League of Robots for a specific cluster. E.g.:
#        $> lor-config nibbler
#  10. Execute playbook to create VMs. E.g.:
#        $> ansible-playbook -i static_inventories/nibbler_hosts.ini deploy-os_servers.yml
#
---
- name: 'Sanity checks before we start.'
  hosts: localhost
  connection: local
  pre_tasks:
    - name: 'Verify Ansible version meets requirements.'
      assert:
        that: "ansible_version.full is version_compare('2.10', '>=')"
        msg: 'You must update Ansible to at least 2.10.x to use this playbook.'
##############################################################################
# Create vxlans, router, security groups and other components we need
# before we can create Virtual Machines.
##############################################################################
- name: Create components required for creating VMs using OpenStack API.
  hosts: localhost
  connection: local
  gather_facts: false
  vars:
    #
    # Disable Ansible's interpretor detection logic,
    # which would fail to use the interpretor from an activated virtual environment.
    #
    - ansible_python_interpreter: python
  tasks:
    - name: "Create {{ network_private_management_id }} network."
      openstack.cloud.network:
        state: present
        name: "{{ network_private_management_id }}"
        external: false
        wait: true
        timeout: "{{ openstack_api_timeout }}"
    - name: "Assign subnet to {{ network_private_management_id }} network."
      openstack.cloud.subnet:
        # 'name' must be the same as 'network_name' or else creating routers for this subnet will fail due to known bug.
        name: "{{ network_private_management_id }}"
        network_name: "{{ network_private_management_id }}"
        state: present
        cidr: "{{ network_private_management_cidr }}"
        # Bug: https://storyboard.openstack.org/#!/story/2008172
        # openstack.cloud.subnet is not idempotent and will fail when a router is present
        # and which is linked to the gateway_ip of the subnet.
        # In that case you must delete the router in order to be able to run this playbook,
        # which may require disassociating floating IPs first: yikes!
        gateway_ip: "{{ network_private_management_gw }}"
        enable_dhcp: true
        dns_nameservers: "{{ nameservers }}"
        wait: true
        timeout: "{{ openstack_api_timeout }}"
      tags:
        - never
    - name: "Create {{ network_private_storage_id }} network."
      openstack.cloud.network:
        name: "{{ network_private_storage_id }}"
        state: present
        external: false
        wait: true
        timeout: "{{ openstack_api_timeout }}"
    - name: "Assign subnet to {{ network_private_storage_id }} network."
      openstack.cloud.subnet:
        # 'name' must be the same as 'network_name' or else creating routers for this subnet will fail due to known bug.
        name: "{{ network_private_storage_id }}"
        network_name: "{{ network_private_storage_id }}"
        state: present
        cidr: "{{ network_private_storage_cidr }}"
        no_gateway_ip: true
        enable_dhcp: true
        dns_nameservers: "{{ nameservers }}"
        wait: true
        timeout: "{{ openstack_api_timeout }}"
    - name: "Create router to bridge {{ network_public_external_id }} and {{ network_private_management_id }} networks."
      openstack.cloud.router:
        state: present
        name: "Router bridging {{ network_public_external_id }} and {{ network_private_management_id }}"
        network: "{{ network_public_external_id }}"
        interfaces:
          - "{{ network_private_management_id }}"
          #
          # Specifying only the network_private_management_id will fail when the default gateway IP
          # from that subnet is already in use. In that case we must specify an IP,
          # but there is no easy, safe way to determine which one we should use....
          #
          # - net: "{{ network_private_management_id }}"
          #   subnet: "{{ network_private_management_id }}"
          #   portip: 10.10.1.1
        wait: true
        timeout: "{{ openstack_api_timeout }}"
    - name: "Create security group for {{ slurm_cluster_name }} jumphosts."
      openstack.cloud.security_group:
        state: present
        name: "{{ slurm_cluster_name }}_jumphosts"
        description: |
                     Security group for security hardened jumphosts bridging the external and internal network.
                     Allows SSH inbound on both port 22 and 443.
                     Allows ICMP inbound.
                     Allows all outbound traffic.
        wait: true
        timeout: "{{ openstack_api_timeout }}"
    - name: "Add rule to {{ slurm_cluster_name }}_jumphosts security group: allow SSH inbound on port 22."
      openstack.cloud.security_group_rule:
        security_group: "{{ slurm_cluster_name }}_jumphosts"
        direction: ingress
        protocol: tcp
        port_range_min: 22
        port_range_max: 22
        remote_ip_prefix: 0.0.0.0/0
        wait: true
        timeout: "{{ openstack_api_timeout }}"
    - name: "Add rule to {{ slurm_cluster_name }}_jumphosts security group: allow SSH inbound on port 443."
      openstack.cloud.security_group_rule:
        security_group: "{{ slurm_cluster_name }}_jumphosts"
        direction: ingress
        protocol: tcp
        port_range_min: 443
        port_range_max: 443
        remote_ip_prefix: 0.0.0.0/0
        wait: true
        timeout: "{{ openstack_api_timeout }}"
    - name: "Add rule to {{ slurm_cluster_name }}_jumphosts security group: allow LDAPS inbound on port 636."
      openstack.cloud.security_group_rule:
        security_group: "{{ slurm_cluster_name }}_jumphosts"
        direction: ingress
        protocol: tcp
        port_range_min: 636
        port_range_max: 636
        remote_ip_prefix: 0.0.0.0/0  # ToDo restrict to {{ ldap_uri }}
        wait: true
        timeout: "{{ openstack_api_timeout }}"
    - name: "Add rule to {{ slurm_cluster_name }}_jumphosts security group: allow ICMP inbound."
      openstack.cloud.security_group_rule:
        security_group: "{{ slurm_cluster_name }}_jumphosts"
        direction: ingress
        protocol: icmp
        port_range_min: -1  # ICMP protocol does not have any ports.
        port_range_max: -1  # ICMP protocol does not have any ports.
        remote_ip_prefix: 0.0.0.0/0
        wait: true
        timeout: "{{ openstack_api_timeout }}"
    - name: Create security group for {{ slurm_cluster_name }} cluster machines behind jumphost.
      openstack.cloud.security_group:
        state: present
        name: "{{ slurm_cluster_name }}_cluster"
        description: |
                     Security group for cluster machines behind a jumphost.
                     Allows SSH and ICMP inbound from machines in the jumphost security group.
                     Allows any traffic inbound from other machines in the same security group.
                     Allows all outbound traffic.
        wait: true
        timeout: "{{ openstack_api_timeout }}"
    - name: "Add rule to {{ slurm_cluster_name }}_cluster security group: allow SSH inbound on port 22 from {{ slurm_cluster_name }}_jumphosts security group."
      openstack.cloud.security_group_rule:
        security_group: "{{ slurm_cluster_name }}_cluster"
        direction: ingress
        protocol: tcp
        port_range_min: 22
        port_range_max: 22
        remote_group: "{{ slurm_cluster_name }}_jumphosts"
        wait: true
        timeout: "{{ openstack_api_timeout }}"
    - name: "Add rule to {{ slurm_cluster_name }}_cluster security group: allow LDAPS inbound on port 636."
      openstack.cloud.security_group_rule:
        security_group: "{{ slurm_cluster_name }}_cluster"
        direction: ingress
        protocol: tcp
        port_range_min: 636
        port_range_max: 636
        remote_ip_prefix: 0.0.0.0/0  # ToDo restrict to {{ ldap_uri }}
        wait: true
        timeout: "{{ openstack_api_timeout }}"
    - name: "Add rule to {{ slurm_cluster_name }}_cluster security group: allow ICMP inbound from {{ slurm_cluster_name }}_jumphosts security group."
      openstack.cloud.security_group_rule:
        security_group: "{{ slurm_cluster_name }}_cluster"
        direction: ingress
        protocol: icmp
        port_range_min: -1  # ICMP protocol does not have any ports.
        port_range_max: -1  # ICMP protocol does not have any ports.
        remote_group: "{{ slurm_cluster_name }}_jumphosts"
        wait: true
        timeout: "{{ openstack_api_timeout }}"
    - name: "Add rule to {{ slurm_cluster_name }}_cluster security group: allow any inbound tcp traffic from machines within the same security group."
      openstack.cloud.security_group_rule:
        security_group: "{{ slurm_cluster_name }}_cluster"
        direction: ingress
        protocol: tcp
        port_range_min: -1  # Port range min -1 and max -1 means the same as min 1 and max 65535,
        port_range_max: -1  # but the latter is not idempotent due to a known bug.
        remote_group: "{{ slurm_cluster_name }}_cluster"
        wait: true
        timeout: "{{ openstack_api_timeout }}"
    - name: "Add rule to {{ slurm_cluster_name }}_cluster security group: allow any inbound udp traffic from machines within the same security group."
      openstack.cloud.security_group_rule:
        security_group: "{{ slurm_cluster_name }}_cluster"
        direction: ingress
        protocol: udp
        port_range_min: -1  # Port range min -1 and max -1 means the same as min 1 and max 65535,
        port_range_max: -1  # but the latter is not idempotent due to a known bug.
        remote_group: "{{ slurm_cluster_name }}_cluster"
        wait: true
        timeout: "{{ openstack_api_timeout }}"
    - name: "Add rule to {{ slurm_cluster_name }}_cluster security group: allow any inbound icmp traffic from machines within the same security group."
      openstack.cloud.security_group_rule:
        security_group: "{{ slurm_cluster_name }}_cluster"
        direction: ingress
        protocol: icmp
        port_range_min: -1  # ICMP protocol does not have any ports.
        port_range_max: -1  # ICMP protocol does not have any ports.
        remote_group: "{{ slurm_cluster_name }}_cluster"
        wait: true
        timeout: "{{ openstack_api_timeout }}"
##############################################################################
# Configure jumphosts from inventory using Openstack API.
##############################################################################
- name: Create jumphosts.
  hosts: jumphost
  connection: local
  gather_facts: false
  vars:
    #
    # Disable Ansible's interpretor detection logic,
    # which fails to use the interpretor from an activated virtual environment
    # and hence fails to find the OpenStackSDK if it was installed in a virtual environment.
    #
    - ansible_python_interpreter: python
  tasks:
    - name: Create jumphost server.
      openstack.cloud.server:
        state: present
        name: "{{ inventory_hostname }}"
        image: "{{ cloud_image }}"
        flavor: "{{ flavor_jumphost }}"
        security_groups: "{{ slurm_cluster_name }}_jumphosts"
        auto_floating_ip: false
        nics:
          - net-name: "{{ network_private_management_id }}"
        userdata: |
          #cloud-config
          password: "{{ cloud_console_pass }}"
          chpasswd: { expire: False }
          #
          # Add each entry to ~/.ssh/authorized_keys for the configured user
          # or the first user defined in the user definition directive.
          #
          ssh_authorized_keys:
          {% for public_key in public_keys_of_local_admins %}  - {{ public_key }}
          {% endfor %}
        availability_zone: "{{ availability_zone }}"
        wait: true
        timeout: "{{ openstack_api_timeout }}"
      register: jumphost_vm
    - name: Assign floating IP to jumphost
      openstack.cloud.floating_ip:
        server: "{{ inventory_hostname }}"
        state: present
        reuse: true
        network: "{{ network_public_external_id }}"
        nat_destination: "{{ network_private_management_id }}"
        wait: true
        timeout: "{{ openstack_api_timeout }}"
      #
      # Known bug https://github.com/ansible/ansible/issues/57451
      # openstack.cloud.floating_ip is not idempotent:
      # Succeeds only the first time and throws error on any subsequent calls.
      # Therefore we use a "when" with a silly complex jinja filter including a JMESpath query
      # to check is the VM already has a floating IP linked to an interface in the correct VXLAN.
      #
      when: (jumphost_vm.server.addresses | dict2items(key_name='vlan', value_name='specs') | json_query(query)) != network_private_management_id
      vars:
        query: '[?specs[?"OS-EXT-IPS:type"==`floating`]].vlan | [0]'
##############################################################################
# Configure repo servers from inventory using Openstack API.
##############################################################################
- name: Create repo servers.
  hosts: repo
  connection: local
  gather_facts: false
  vars:
    #
    # Disable Ansible's interpretor detection logic,
    # which fails to use the interpretor from an activated virtual environment
    # and hence fails to find the OpenStackSDK if it was installed in a virtual environment.
    #
    - ansible_python_interpreter: python
  tasks:
    - name: Create persistent data volume for repo server.
      openstack.cloud.volume:
        display_name: "{{ inventory_hostname }}-volume"
        size: "{{ local_volume_size_repo }}"
        state: present
        availability_zone: "{{ storage_availability_zone | default('nova') }}"
        wait: true
        timeout: "{{ openstack_api_timeout }}"
    - name: Create repo server.
      openstack.cloud.server:
        state: present
        name: "{{ inventory_hostname }}"
        image: "{{ cloud_image }}"
        flavor: "{{ flavor_management }}"
        security_groups: "{{ slurm_cluster_name }}_cluster"
        auto_floating_ip: false
        nics:
          - net-name: "{{ network_private_management_id }}"
        userdata: |
          #cloud-config
          password: "{{ cloud_console_pass }}"
          chpasswd: { expire: False }
          #
          # Add each entry to ~/.ssh/authorized_keys for the configured user
          # or the first user defined in the user definition directive.
          #
          ssh_authorized_keys:
          {% for public_key in public_keys_of_local_admins %}  - {{ public_key }}
          {% endfor %}
        availability_zone: "{{ availability_zone }}"
        wait: true
        timeout: "{{ openstack_api_timeout }}"
      register: repo_vm
##############################################################################
# Configure UIs from inventory using Openstack API.
##############################################################################
- name: Create User Interfaces.
  hosts: user_interface
  connection: local
  gather_facts: false
  vars:
    #
    # Disable Ansible's interpretor detection logic,
    # which fails to use the interpretor from an activated virtual environment
    # and hence fails to find the OpenStackSDK if it was installed in a virtual environment.
    #
    - ansible_python_interpreter: python
  tasks:
    - name: Create UI server.
      openstack.cloud.server:
        state: present
        name: "{{ inventory_hostname }}"
        image: "{{ cloud_image }}"
        flavor: "{{ flavor_ui }}"
        security_groups: "{{ slurm_cluster_name }}_cluster"
        auto_floating_ip: false
        nics:
          - net-name: "{{ network_private_management_id }}"
          - net-name: "{{ network_private_storage_id }}"
        userdata: |
          #cloud-config
          password: "{{ cloud_console_pass }}"
          chpasswd: { expire: False }
          #
          # Add each entry to ~/.ssh/authorized_keys for the configured user
          # or the first user defined in the user definition directive.
          #
          ssh_authorized_keys:
          {% for public_key in public_keys_of_local_admins %}  - {{ public_key }}
          {% endfor %}
        availability_zone: "{{ availability_zone }}"
        wait: true
        timeout: "{{ openstack_api_timeout }}"
##############################################################################
# Configure compute nodes from inventory using Openstack API.
##############################################################################
- name: Create compute nodes.
  hosts:
    - compute_vm
  connection: local
  gather_facts: false
  vars:
    #
    # Disable Ansible's interpretor detection logic,
    # which fails to use the interpretor from an activated virtual environment
    # and hence fails to find the OpenStackSDK if it was installed in a virtual environment.
    #
    - ansible_python_interpreter: python
  tasks:
    - name: Create persistent data volume for compute node.
      openstack.cloud.volume:
        display_name: "{{ inventory_hostname }}-volume"
        size: "{{ local_volume_size_vcompute }}"
        state: present
        availability_zone: "{{ storage_availability_zone | default('nova') }}"
        wait: true
        timeout: "{{ openstack_api_timeout }}"
    - name: Create compute node server.
      openstack.cloud.server:
        state: present
        name: "{{ inventory_hostname }}"
        image: "{{ cloud_image }}"
        flavor: "{{ flavor_vcompute }}"
        security_groups: "{{ slurm_cluster_name }}_cluster"
        auto_floating_ip: false
        nics:
          - net-name: "{{ network_private_management_id }}"
          - net-name: "{{ network_private_storage_id }}"
        userdata: |
          #cloud-config
          password: "{{ cloud_console_pass }}"
          chpasswd: { expire: False }
          #
          # Add each entry to ~/.ssh/authorized_keys for the configured user
          # or the first user defined in the user definition directive.
          #
          ssh_authorized_keys:
          {% for public_key in public_keys_of_local_admins %}  - {{ public_key }}
          {% endfor %}
        availability_zone: "{{ availability_zone }}"
        wait: true
        timeout: "{{ openstack_api_timeout }}"
    - name: Attach local storage volume to compute node.
      openstack.cloud.server_volume:
        server: "{{ inventory_hostname }}"
        volume: "{{ inventory_hostname }}-volume"
        wait: true
        timeout: "{{ openstack_api_timeout }}"
#############################################################################
# Configure DAI and SAI from inventory using Openstack API.
#############################################################################
- name: Create Sys and Deploy Admin Interfaces.
  hosts:
    - sys_admin_interface
    - deploy_admin_interface
  connection: local
  gather_facts: false
  vars:
    #
    # Disable Ansible's interpretor detection logic,
    # which fails to use the interpretor from an activated virtual environment
    # and hence fails to find the OpenStackSDK if it was installed in a virtual environment.
    #
    - ansible_python_interpreter: python
  tasks:
    - name: Create persistent data volume for admin interface.
      openstack.cloud.volume:
        display_name: "{{ inventory_hostname }}-volume"
        size: "{{ local_volume_size_management }}"
        state: present
        availability_zone: "{{ storage_availability_zone | default('nova') }}"
        wait: true
        timeout: "{{ openstack_api_timeout }}"
    - name: Create admin interface server.
      openstack.cloud.server:
        state: present
        name: "{{ inventory_hostname }}"
        image: "{{ cloud_image }}"
        flavor: "{{ flavor_management }}"
        security_groups: "{{ slurm_cluster_name }}_cluster"
        auto_floating_ip: false
        nics:
          - net-name: "{{ network_private_management_id }}"
          - net-name: "{{ network_private_storage_id }}"
        userdata: |
          #cloud-config
          password: "{{ cloud_console_pass }}"
          chpasswd: { expire: False }
          #
          # Add each entry to ~/.ssh/authorized_keys for the configured user
          # or the first user defined in the user definition directive.
          #
          ssh_authorized_keys:
          {% for public_key in public_keys_of_local_admins %}  - {{ public_key }}
          {% endfor %}
        availability_zone: "{{ availability_zone }}"
        wait: true
        timeout: "{{ openstack_api_timeout }}"
    - name: Attach local storage volume to admin interface server.
      openstack.cloud.server_volume:
        server: "{{ inventory_hostname }}"
        volume: "{{ inventory_hostname }}-volume"
        wait: true
        timeout: "{{ openstack_api_timeout }}"
##############################################################################
# Get IPs addresses from API for static hostname lookup with /etc/hosts.
##############################################################################
- name: Fetch network addresses assigned to VMs using OpenStack API.
  hosts: localhost
  connection: local
  gather_facts: false
  vars:
    #
    # Disable Ansible's interpretor detection logic,
    # which would fail to use the interpretor from an activated virtual environment.
    #
    - ansible_python_interpreter: python
  tasks:
    #
    # Note: we fetch info for all servers relevant or not
    # as filtering directly during the API call is problematic.
    # Will filter the results for the relevant servers later on.
    #
    - name: Get server info from OpenStack API.
      openstack.cloud.server_info:
      register: api_server_info
    - name: "Add addresses to {{ playbook_dir }}/group_vars/{{ slurm_cluster_name }}_cluster/ip_addresses.yml"
      template:
        src: "{{ playbook_dir }}/group_vars/template/ip_addresses.yml.j2"
        dest: "{{ playbook_dir }}/group_vars/{{ slurm_cluster_name }}_cluster/ip_addresses.yml.new"
        mode: '0644'
      vars:
        relevant_servers_list: "{{ groups['jumphost'] | default([]) + groups['repo'] | default([]) + groups['cluster'] | default([]) }}"
        relevant_servers_info: "{{ api_server_info.openstack_servers | selectattr('name', 'in', relevant_servers_list) | list }}"
    - name: "ToDo"
      debug:
        msg: |
             ***********************************************************************************************************
             IMPORTANT: Manual work!
                        Ansible created:
                            {{ playbook_dir }}/group_vars/{{ slurm_cluster_name }}_cluster/ip_addresses.yml.new
                        Please inspect this file carefully with:
                            diff -y {{ playbook_dir }}/group_vars/{{ slurm_cluster_name }}_cluster/ip_addresses.yml{.new,}
                        and if Ok execute:
                            mv {{ playbook_dir }}/group_vars/{{ slurm_cluster_name }}_cluster/ip_addresses.yml{.new,}
             ***********************************************************************************************************
...
